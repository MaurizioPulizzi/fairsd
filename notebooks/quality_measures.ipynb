{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oriental-lingerie",
   "metadata": {},
   "source": [
    "# Qaulity Measures\n",
    "For this example a toy dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset creation\n",
    "import pandas as pd\n",
    "X = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\n",
    "y_true = pd.Series([1, 0, 1])\n",
    "y_pred = pd.Series([1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-newsletter",
   "metadata": {},
   "source": [
    "## Fairlearn Quality measures\n",
    "All the Fairlear metrics can be used as quality measure. See the Fairlearn documentation [here](https://fairlearn.github.io/v0.6.0/api_reference/fairlearn.metrics.html).<br/>\n",
    "**The predefined fairlearn metrics are:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-sudan",
   "metadata": {},
   "source": [
    "|Metric name | Description\n",
    "|:-----|:-----\n",
    "| demographic_parity_difference |  Defined as the absolute value of the difference in the **selection rates** between a subgroup and its negation.\n",
    "|demographic_parity_ratio | Defined as the ratio between the smallest and the largest group-level **selection rate**, between a subgroup and its negation.\n",
    "|equalized_odds_difference | The greater of two metrics: true_positive_rate_difference and false_positive_rate_difference. The former is the difference between the TPRs, between a subgroup and its negation. The latter is defined similarly, but for FPRs.\n",
    "|equalized_odds_ratio | The smaller of two metrics: true_positive_rate_ratio and false_positive_rate_ratio. The former is the ratio between the TPRs, between a subgroup and its negation. The latter is defined similarly, but for FPRs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-broadcasting",
   "metadata": {},
   "source": [
    "We can inizialize a SubgroupDiscoveryTask object in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "northern-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairsd as fsd\n",
    "from fairlearn.metrics import demographic_parity_ratio\n",
    "task = fsd.SubgroupDiscoveryTask(X, y_true, y_pred, demographic_parity_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-munich",
   "metadata": {},
   "source": [
    "Or, faster, we can pass the same metric as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infrared-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = fsd.SubgroupDiscoveryTask(X, y_true, y_pred, 'demographic_parity_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-carrier",
   "metadata": {},
   "source": [
    "From the version 6.0, Fairlearn also offers the interesting possibility of \"create a scalar returning metric function based on aggregation of a disaggregated metric\".<br/>\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "general-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from fairlearn.metrics import make_derived_metric\n",
    "derived_metric = make_derived_metric(metric = accuracy_score, transform = 'difference')\n",
    "task = fsd.SubgroupDiscoveryTask(X, y_true, y_pred, derived_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-nutrition",
   "metadata": {},
   "source": [
    "For more details see the Fairlearn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-tonight",
   "metadata": {},
   "source": [
    "## Customized Quality Measures\n",
    "It is possible to create a quality measure by estending the class [QualityFunction](https://github.com/MaurizioPulizzi/fairsd/blob/main/fairsd/qualitymeasures.py#L3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "renewable-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyQualityMeasure(fsd.QualityFunction):\n",
    "    def evaluate(self, y_true = None, y_pred=None, sensitive_features=None):\n",
    "        return 0.5\n",
    "    \n",
    "task = fsd.SubgroupDiscoveryTask(X, y_true, y_pred, MyQualityMeasure.evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-crown",
   "metadata": {},
   "source": [
    "# Descriptions and Quality Measures\n",
    "A subgroup description is formed by the conjunction of zero or more descriptors.<br/>\n",
    "A descriptor is a statement in the form \"attribute_name = attribute_value\" for nomilal attributes or \"attribute_name = range\" for numerical attributes.<br/>\n",
    "Example of Description: \" sex = 'Male' AND age = (18, 30] \". <br/>\n",
    "The Top-k subgroup discovery task in this package returns the k subgroup descriptions of the subgroups that exert the greatest disparity.<br>\n",
    "There is no single definition of subgroup disparity, the meaning changes according to the used quality measure.\n",
    "\n",
    "**All metrics in the [fairlearn.metrics](https://fairlearn.github.io/v0.6.0/api_reference/fairlearn.metrics.html) module are symmetrical:** they always return a value between 0 and 1 and do not distinguish whether a subgroup is \"positively\" or \"negatively\" dissimilar. For example the descriptions \"married = True\" and \"married = False\" will always have the same quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
